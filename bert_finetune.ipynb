{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb60e6e-ebd4-4a2d-96f8-8b5f2c65d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#', 'type', 'text', 'original_label', 'aggregatedAnnotationConfidence',\n",
       "       'annotator1', 'annotator2', 'annotator3'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>original_label</th>\n",
       "      <th>aggregatedAnnotationConfidence</th>\n",
       "      <th>annotator1</th>\n",
       "      <th>annotator2</th>\n",
       "      <th>annotator3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TWEET</td>\n",
       "      <td>مبروك و سامحونا لعجزنا التام. عقبال اللي جوه. اللي بره يا عاجز يا بيزايد على العاجز</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C1</td>\n",
       "      <td>كلنا بره ومش هنبطل نزايد على العجايز الي جابونا ورى</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C2</td>\n",
       "      <td>بدل ما انت قاعد بره كده تعالي ازرع الصحرا</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C3</td>\n",
       "      <td>قذر اتفووو ماتيجى مصر وتورينا نفسك كدا ياجبان</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #   type   \n",
       "0  1  TWEET  \\\n",
       "1  2     C1   \n",
       "2  3     C2   \n",
       "3  4     C3   \n",
       "\n",
       "                                                                                  text   \n",
       "0  مبروك و سامحونا لعجزنا التام. عقبال اللي جوه. اللي بره يا عاجز يا بيزايد على العاجز  \\\n",
       "1                                  كلنا بره ومش هنبطل نزايد على العجايز الي جابونا ورى   \n",
       "2                                            بدل ما انت قاعد بره كده تعالي ازرع الصحرا   \n",
       "3                                        قذر اتفووو ماتيجى مصر وتورينا نفسك كدا ياجبان   \n",
       "\n",
       "   original_label  aggregatedAnnotationConfidence  annotator1  annotator2   \n",
       "0               0                          0.6667          -1           0  \\\n",
       "1              -1                          0.6667          -1          -1   \n",
       "2               0                          1.0000           0           0   \n",
       "3              -1                          1.0000          -1          -1   \n",
       "\n",
       "   annotator3  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3          -1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{-2, -1, 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 880\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 220\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='275' max='275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [275/275 00:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.066100</td>\n",
       "      <td>1.043688</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.199575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.043900</td>\n",
       "      <td>1.049138</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.199575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.039700</td>\n",
       "      <td>1.026412</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.337077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.036800</td>\n",
       "      <td>1.034759</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.319221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.045800</td>\n",
       "      <td>1.030766</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.329023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.001300</td>\n",
       "      <td>1.019175</td>\n",
       "      <td>0.495455</td>\n",
       "      <td>0.359691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.050300</td>\n",
       "      <td>1.043233</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.342471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.038800</td>\n",
       "      <td>1.030905</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.304056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.004500</td>\n",
       "      <td>1.032683</td>\n",
       "      <td>0.413636</td>\n",
       "      <td>0.197611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.037200</td>\n",
       "      <td>1.012174</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.358935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>1.033813</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.338659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>1.076193</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.319106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.977500</td>\n",
       "      <td>1.069556</td>\n",
       "      <td>0.459091</td>\n",
       "      <td>0.335681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.067000</td>\n",
       "      <td>1.045079</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.308713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.956200</td>\n",
       "      <td>1.034403</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.302083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.033300</td>\n",
       "      <td>1.032525</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.337902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.007500</td>\n",
       "      <td>1.034616</td>\n",
       "      <td>0.504545</td>\n",
       "      <td>0.368842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.055500</td>\n",
       "      <td>1.021562</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.350668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.974300</td>\n",
       "      <td>1.019399</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.346836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.960300</td>\n",
       "      <td>1.038090</td>\n",
       "      <td>0.495455</td>\n",
       "      <td>0.361868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.953500</td>\n",
       "      <td>1.032399</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.346197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.963900</td>\n",
       "      <td>1.036504</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.332969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>1.041995</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.349508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.955700</td>\n",
       "      <td>1.043931</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.353533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.974300</td>\n",
       "      <td>1.044919</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.346219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.939900</td>\n",
       "      <td>1.043776</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.349272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>1.045012</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.346009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=275, training_loss=0.998851318359375, metrics={'train_runtime': 12.3631, 'train_samples_per_second': 355.899, 'train_steps_per_second': 22.244, 'total_flos': 289424759500800.0, 'train_loss': 0.998851318359375, 'epoch': 5.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('dataset.csv', encoding='utf-8', engine='python') #, sep='\\t' , quotechar=\"'\"  , quoting=3\n",
    "display(df.columns)\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "display(df[:4])\n",
    "\n",
    "\n",
    "\n",
    "df = df[df['text'] != '']  # the name of field contains the text (adjust accordingly) \n",
    "\n",
    "classes = set(df['original_label'].values) # the name of field contains the label (adjust accordingly)\n",
    "display(classes)\n",
    "\n",
    "df['original_label'] = df['original_label'].astype('category')\n",
    "df['label'] = df['original_label'].cat.codes  # keep the name 'label' as it is (do not change)\n",
    "\n",
    "\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "\n",
    "classes_num = len(classes)\n",
    "display(classes_num)\n",
    "display(len(df))\n",
    "\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "display(ds)\n",
    "\n",
    "max_sequence_length = 128 # you can change to 64 if the text is short\n",
    "\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "              \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                              num_labels=classes_num).to('cuda')                                                 \n",
    "dataset_train = ds['train']\n",
    "dataset_validation = ds['test']                                                    \n",
    "        \n",
    "      \n",
    "\n",
    "def preprocess_function(examples):\n",
    "            return tokenizer(examples['text'], truncation=True, padding=\"max_length\",  # change 'text' to the name of the text column if it's different\n",
    "                            max_length=max_sequence_length, add_special_tokens=True)\n",
    "        \n",
    "        \n",
    "dataset_train = dataset_train.map(preprocess_function, batched=True)\n",
    "dataset_validation = dataset_validation.map(preprocess_function, batched=True)\n",
    "        \n",
    "       \n",
    "        \n",
    "def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            predictions = np.argmax(logits, axis=-1)    \n",
    "            acc = accuracy_score(labels, predictions)        \n",
    "            f1 = f1_score(labels, predictions, average='macro')               \n",
    "            return {'accuracy': acc, 'f1_score': f1}\n",
    "\n",
    "            \n",
    "epochs = 5  # increase for more training time\n",
    "save_steps = 10000 #save checkpoint every 10000 steps\n",
    "batch_size = 16 # change to 32 if gpu's memory allows it\n",
    "        \n",
    "training_args = TrainingArguments(\n",
    "            output_dir = 'bert/',\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs = epochs,\n",
    "            per_device_train_batch_size = batch_size,\n",
    "            per_device_eval_batch_size = batch_size,\n",
    "            save_steps = save_steps,\n",
    "            save_total_limit = 1, #only save the last 5 checkpoints\n",
    "            fp16=True,\n",
    "            learning_rate = 5e-5,  # 5e-5 is the default\n",
    "            logging_steps = 10, #50_000\n",
    "            evaluation_strategy = 'steps',\n",
    "            eval_steps = 10\n",
    "            \n",
    "        )\n",
    "        \n",
    "trainer = Trainer(\n",
    "            model = model,\n",
    "            args = training_args,\n",
    "            train_dataset=dataset_train,\n",
    "            eval_dataset=dataset_validation,\n",
    "            compute_metrics = compute_metrics\n",
    "        )\n",
    "        \n",
    "        \n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6f4ec-e6be-4eaf-8171-913e9ec11e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604249b-3df4-48bf-962d-b14d3911709b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f6179-e8f0-4e55-888e-d38cc70a6cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354374a-7d4d-48f5-8ced-1784ef469388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
